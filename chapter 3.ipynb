{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.231792Z",
     "start_time": "2025-01-13T09:02:49.418285Z"
    }
   },
   "source": "import torch",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.244748Z",
     "start_time": "2025-01-13T09:02:54.233801Z"
    }
   },
   "cell_type": "code",
   "source": [
    " inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your    (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts  (x^3)\n",
    "   [0.22, 0.58, 0.33], # with    \n",
    "   [0.77, 0.25, 0.10], # one     \n",
    "   [0.05, 0.80, 0.55]] # step    \n",
    ")"
   ],
   "id": "31948935da05b816",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.253736Z",
     "start_time": "2025-01-13T09:02:54.245757Z"
    }
   },
   "cell_type": "code",
   "source": "'''  The first step of implementing self-attention is to compute the intermediate values ω, referred to as attention scores '''",
   "id": "9884d4a565afa776",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  The first step of implementing self-attention is to compute the intermediate values ω, referred to as attention scores '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.312743Z",
     "start_time": "2025-01-13T09:02:54.255745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = inputs[1]\n",
    "attn_score_2 = torch.empty(inputs.shape[0])\n",
    "print(f\"attn_score_2: {attn_score_2}\")\n",
    "for i ,x_i in enumerate(inputs):\n",
    "    attn_score_2[i] = torch.dot(x_i,query)\n",
    "print(f\"attn_score_2: {attn_score_2}\")"
   ],
   "id": "a32354adaa6dd2c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_score_2: tensor([4.2855e-14, 2.0445e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])\n",
      "attn_score_2: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.320127Z",
     "start_time": "2025-01-13T09:02:54.315262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" In the con\n",
    "text of self-attention mechanisms, the dot product determines the extent to which\n",
    " each element in a sequence focuses on, or “attends to,” any other element: the\n",
    " higher the dot product, the higher the similarity and attention score between two\n",
    " elements\"\"\""
   ],
   "id": "f489145396c88716",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In the con\\ntext of self-attention mechanisms, the dot product determines the extent to which\\n each element in a sequence focuses on, or “attends to,” any other element: the\\n higher the dot product, the higher the similarity and attention score between two\\n elements'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.330513Z",
     "start_time": "2025-01-13T09:02:54.321142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_weights_2_tmp = attn_score_2 / attn_score_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())\n",
    "'''These are alphas\n",
    " In practice, it’s more common and advisable to use the softmax function for normalization. This approach is better at managing extreme values and offers more favorable gradient properties during training. \n",
    "\n",
    "'''"
   ],
   "id": "811d45285eb1ec02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'These are alphas\\n In practice, it’s more common and advisable to use the softmax function for normalization. This approach is better at managing extreme values and offers more favorable gradient properties during training. \\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.341520Z",
     "start_time": "2025-01-13T09:02:54.331524Z"
    }
   },
   "cell_type": "code",
   "source": [
    " def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    " attn_weights_2_naive = softmax_naive(attn_score_2)\n",
    " print(\"Attention weights:\", attn_weights_2_naive)\n",
    " print(\"Sum:\", attn_weights_2_naive.sum())"
   ],
   "id": "685ed1e1e406ce84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.351007Z",
     "start_time": "2025-01-13T09:02:54.342024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_weights_2 = torch.softmax(attn_score_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ],
   "id": "3efae5206990b90d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.359491Z",
     "start_time": "2025-01-13T09:02:54.352011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ],
   "id": "1433e0ee4eb2d37b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.366495Z",
     "start_time": "2025-01-13T09:02:54.360496Z"
    }
   },
   "cell_type": "code",
   "source": "\"\"\" So far, we have computed attention weights and the context vector for input 2, as shown in the highlighted row in figure 3.11. Now let’s extend this computation to calculate attention weights and context vectors for all inputs.\"\"\"\n",
   "id": "f0146ef771c382a5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' So far, we have computed attention weights and the context vector for input 2, as shown in the highlighted row in figure 3.11. Now let’s extend this computation to calculate attention weights and context vectors for all inputs.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "feadaa7da1b76763"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.375003Z",
     "start_time": "2025-01-13T09:02:54.366999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ],
   "id": "ac0a079f0b10c008",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.385752Z",
     "start_time": "2025-01-13T09:02:54.376513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "attn_scores = torch.empty(6, 6)\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ],
   "id": "2829ef8672489bbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 5 ms\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.392828Z",
     "start_time": "2025-01-13T09:02:54.386757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ],
   "id": "f7f34d668bb70cd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.404179Z",
     "start_time": "2025-01-13T09:02:54.397835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(attn_weights.T[0].sum())\n",
    "print(attn_weights[0].sum())\n"
   ],
   "id": "28d17891ecbed4c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9220)\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.413185Z",
     "start_time": "2025-01-13T09:02:54.406187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_context_vecs = torch.matmul(attn_weights, inputs)\n",
    "print(all_context_vecs)"
   ],
   "id": "347db967c91ce939",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Till now.\n",
    "**Query vector -> Attn score -> Attn weights -> Context vector**\n",
    "\n"
   ],
   "id": "642a76b0056dc287"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.420701Z",
     "start_time": "2025-01-13T09:02:54.414699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''We will implement the self-attention mechanism step by step by introducing the\n",
    " three trainable weight matrices Wq, Wk, and Wv. These three matrices are used to\n",
    " project the embedded input tokens, x(i), into query, key, and value vectors'''"
   ],
   "id": "b58ea6e2cd6daf5a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We will implement the self-attention mechanism step by step by introducing the\\n three trainable weight matrices Wq, Wk, and Wv. These three matrices are used to\\n project the embedded input tokens, x(i), into query, key, and value vectors'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.427317Z",
     "start_time": "2025-01-13T09:02:54.422218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_2 = inputs[1]    \n",
    "d_in = inputs.shape[1]     \n",
    "d_out = 2  "
   ],
   "id": "41feeb8d6b07ac34",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.447324Z",
     "start_time": "2025-01-13T09:02:54.429324Z"
    }
   },
   "cell_type": "code",
   "source": [
    " torch.manual_seed(123)\n",
    " W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    " W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    " W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ],
   "id": "c60107443bf65c8d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.458091Z",
     "start_time": "2025-01-13T09:02:54.448837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_2 = x_2 @ W_query \n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ],
   "id": "9f70ed57387e9845",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.466365Z",
     "start_time": "2025-01-13T09:02:54.459099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape  , keys)\n",
    "print(\"values.shape:\", values.shape  , values)"
   ],
   "id": "1307d8a0edd27dee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2]) tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]])\n",
      "values.shape: torch.Size([6, 2]) tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.474373Z",
     "start_time": "2025-01-13T09:02:54.468373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' The unscaled attention score is computed\n",
    " as a dot product between the query and\n",
    " the key vectors.'''"
   ],
   "id": "37367da89302ffdf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The unscaled attention score is computed\\n as a dot product between the query and\\n the key vectors.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.481886Z",
     "start_time": "2025-01-13T09:02:54.475895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "keys_2 = keys[1]            \n",
    "attn_score_22 = torch.dot(query_2 , keys_2)\n",
    "print(attn_score_22)"
   ],
   "id": "5af2d4b14a943dc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.489334Z",
     "start_time": "2025-01-13T09:02:54.483405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ],
   "id": "4ded7f025ceebc5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.496345Z",
     "start_time": "2025-01-13T09:02:54.491341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"now we scale the attention scores by dividing\n",
    " them by the square root of the embedding dimension of the keys (taking the square\n",
    " root is mathematically the same as exponentiating by 0.5):\"\"\""
   ],
   "id": "6874348c0508a5c7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'now we scale the attention scores by dividing\\n them by the square root of the embedding dimension of the keys (taking the square\\n root is mathematically the same as exponentiating by 0.5):'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.504418Z",
     "start_time": "2025-01-13T09:02:54.498420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_k = keys.shape[-1]\n",
    "print(d_k)\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2 , attn_weights_2.sum())"
   ],
   "id": "8276f1b5d386e252",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820]) tensor(1.)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Rationale Behind Scaled-Dot Product Attention\n",
    "\n",
    "The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like large language models (LLMs), large dot products can result in very small gradients during backpropagation due to the softmax function applied to them.\n",
    "\n",
    "As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate.\n",
    "\n",
    "The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called **scaled-dot product attention**.\n"
   ],
   "id": "3e5b91946589d2ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.512401Z",
     "start_time": "2025-01-13T09:02:54.505935Z"
    }
   },
   "cell_type": "code",
   "source": [
    " context_vec_2 = attn_weights_2 @ values\n",
    " print(context_vec_2)"
   ],
   "id": "8917359e33f79cde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.521771Z",
     "start_time": "2025-01-13T09:02:54.513409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn \n",
    "class SelfAttention_V1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))    \n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    def forward(self, x):\n",
    "        keys = torch.matmul(x, self.W_key)\n",
    "        values = torch.matmul(x, self.W_value)\n",
    "        query = torch.matmul(x, self.W_query)\n",
    "        attn_score = torch.matmul(query, keys.T)\n",
    "        attn_weights = torch.softmax(attn_score/keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = torch.matmul(attn_weights, values)\n",
    "        return context_vec\n"
   ],
   "id": "d068685e1df1b352",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.538243Z",
     "start_time": "2025-01-13T09:02:54.522777Z"
    }
   },
   "cell_type": "code",
   "source": [
    " torch.manual_seed(123)\n",
    " sa_v1 = SelfAttention_V1(d_in, d_out)\n",
    " print(sa_v1(inputs))"
   ],
   "id": "9adc18b84fb27ac0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.545284Z",
     "start_time": "2025-01-13T09:02:54.539249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn \n",
    "class SelfAttention_V2(nn.Module):\n",
    "    def __init__(self, d_in, d_out , qkv_bais = False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bais)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias = qkv_bais)    \n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bais)\n",
    "    def forward(self, x):\n",
    "        query = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_score = torch.matmul(query, keys.T)\n",
    "        attn_weights = torch.softmax(attn_score/keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = torch.matmul(attn_weights, values)\n",
    "        return context_vec\n"
   ],
   "id": "62853245200a2522",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.559354Z",
     "start_time": "2025-01-13T09:02:54.546289Z"
    }
   },
   "cell_type": "code",
   "source": [
    " torch.manual_seed(789)\n",
    " sa_v2 = SelfAttention_V2(d_in, d_out)\n",
    " print(sa_v2(inputs))"
   ],
   "id": "400e6b71a844cb82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Now enhancements to the self-attention mechanism, focusing specifically on incorporating causal and multi-head elements.",
   "id": "e3a360d09d1ea3d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Causal Aspect and Multi-Head Attention\n",
    "\n",
    "### Causal Aspect/ masked atten:\n",
    "The causal aspect involves modifying the attention mechanism to prevent the model from accessing future information in the sequence. This is crucial for tasks like **language modeling**, where each word prediction should only depend on previous words. By masking the future tokens, the model ensures it generates predictions in a left-to-right manner, preserving the causal relationship in the sequence.\n",
    "\n",
    "### Multi-Head Attention:\n",
    "The **multi-head** component involves splitting the attention mechanism into multiple \"heads.\" Each head learns different aspects of the data, allowing the model to simultaneously attend to information from different representation subspaces at different positions. This enables the model to capture a richer set of dependencies, improving its performance in complex tasks by considering various perspectives at once.\n"
   ],
   "id": "17146c987899b1c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.568707Z",
     "start_time": "2025-01-13T09:02:54.561363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "queries = sa_v2.W_query(inputs)    \n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ],
   "id": "f62f624d4db5dc11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.577714Z",
     "start_time": "2025-01-13T09:02:54.570715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)\n"
   ],
   "id": "5db177db5fb22752",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.584995Z",
     "start_time": "2025-01-13T09:02:54.579186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ],
   "id": "767668812e182f2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.593046Z",
     "start_time": "2025-01-13T09:02:54.587004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "print(row_sums)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ],
   "id": "50d46583eb370f3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921],\n",
      "        [0.3700],\n",
      "        [0.5357],\n",
      "        [0.6775],\n",
      "        [0.8415],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.606055Z",
     "start_time": "2025-01-13T09:02:54.594053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(mask)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ],
   "id": "cae82d67b01d9626",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.614569Z",
     "start_time": "2025-01-13T09:02:54.607568Z"
    }
   },
   "cell_type": "code",
   "source": [
    " attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    " print(attn_weights)"
   ],
   "id": "23c568e977128c74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Masking additional attention weights with dropout\n",
    "In the transformer architecture, including models like GPT, dropout in the atten\n",
    "tion mechanism is typically applied at two specific times: after calculating the atten\n",
    "tion weights or after applying the attention weights to the value vectors. Here we will\n",
    " apply the dropout mask after computing the attention weights,"
   ],
   "id": "7b567bd790dead59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.631027Z",
     "start_time": "2025-01-13T09:02:54.616088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)   \n",
    "example = torch.ones(6, 6)     \n",
    "print(dropout(example))"
   ],
   "id": "4af6c3ada9aac714",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.643198Z",
     "start_time": "2025-01-13T09:02:54.632034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ],
   "id": "e88c66a104fc5d40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.651432Z",
     "start_time": "2025-01-13T09:02:54.645207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch = torch.stack((inputs , inputs) , dim = 0 )\n",
    "print(batch.shape)"
   ],
   "id": "c2aa3a5ea28e8012",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.662921Z",
     "start_time": "2025-01-13T09:02:54.652440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CasualAtten(nn.Module):\n",
    "    def __init__(self , d_in , d_out , context_len , dropout,qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)           \n",
    "        self.register_buffer(\n",
    "          'mask',\n",
    "          torch.triu(torch.ones(context_length, context_length),\n",
    "          diagonal=1)\n",
    "        )   \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape                  \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)  \n",
    "        attn_scores = queries @ keys.transpose(1, 2)   \n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "         )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        \n",
    "        return context_vec"
   ],
   "id": "532ed3b07ae8f67c",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.685042Z",
     "start_time": "2025-01-13T09:02:54.670930Z"
    }
   },
   "cell_type": "code",
   "source": [
    " torch.manual_seed(123)\n",
    " context_length = batch.shape[1]\n",
    " ca = CasualAtten(d_in, d_out, context_length, 0.0)\n",
    " context_vecs = ca(batch)\n",
    " print(\"context_vecs.shape:\", context_vecs.shape)\n"
   ],
   "id": "1f7e92c255dc37a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.693304Z",
     "start_time": "2025-01-13T09:02:54.687050Z"
    }
   },
   "cell_type": "code",
   "source": [
    " class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out,context_len ,dropout,num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CasualAtten(\n",
    "            d_in, d_out, context_length, dropout, qkv_bias) \n",
    "            for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.concat([head(x) for head in self.heads], dim=-1)"
   ],
   "id": "a97c276cfea2f1e",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.708336Z",
     "start_time": "2025-01-13T09:02:54.694316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length,dropout= 0.0, num_heads=2\n",
    " )\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ],
   "id": "46995bba6b51dc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.715343Z",
     "start_time": "2025-01-13T09:02:54.709342Z"
    }
   },
   "cell_type": "code",
   "source": [
    " a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],   \n",
    "            [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "            [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "           [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "            [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "            [0.4606, 0.5159, 0.4220, 0.5786]]]])"
   ],
   "id": "552e5e686cd50705",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.723318Z",
     "start_time": "2025-01-13T09:02:54.716846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(a@a.transpose(2,3))\n",
    "print(\"a.shape:\", a.shape)\n",
    "print(\"a:\", (a@a.transpose(2,3)).shape)"
   ],
   "id": "70fc19cddf0d1c58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n",
      "a.shape: torch.Size([1, 2, 3, 4])\n",
      "a: torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:02:54.730348Z",
     "start_time": "2025-01-13T09:02:54.725326Z"
    }
   },
   "cell_type": "code",
   "source": "print((a.transpose(2,3)).shape)",
   "id": "ad0f4524bd6db793",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4, 3])\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:18:40.477371Z",
     "start_time": "2025-01-13T09:18:40.467253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_len,dropout,num_heads , qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out% num_heads==0) , \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads   \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)   \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "         )\n",
    "    def forward(self, x):\n",
    "        batch , num_tokens, d_in = x.shape \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        keys = keys.view(batch , num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(batch , num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch , num_tokens, self.num_heads, self.head_dim)\n",
    "        keys  = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vecs = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vecs = context_vecs.contiguous().view(batch, num_tokens, self.d_out)\n",
    "        context_vecs = self.out_proj(context_vecs)\n",
    "        return context_vecs"
   ],
   "id": "c7a689bd49f9ae4c",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:19:08.546290Z",
     "start_time": "2025-01-13T09:19:08.537853Z"
    }
   },
   "cell_type": "code",
   "source": [
    " first_head = a[0, 0, :, :]\n",
    " first_res = first_head @ first_head.T\n",
    " print(\"First head:\\n\", first_res)\n",
    " second_head = a[0, 1, :, :]\n",
    " second_res = second_head @ second_head.T\n",
    " print(\"\\nSecond head:\\n\", second_res)"
   ],
   "id": "3ab7d10e4a2e4d21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:21:23.549408Z",
     "start_time": "2025-01-13T09:21:23.535495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 768\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=12)\n",
    "context_vecs = mha(batch)\n",
    "# print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ],
   "id": "e4919a150857ad2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 768])\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4133e3fb026318b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
